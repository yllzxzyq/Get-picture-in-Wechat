{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成广播变量rdd.broadcast()\n",
    "通过broadcast()方法可以在Spark各个分区都生成一个新的rdd，在各个分区都可以通过rdd.value调用他，因而避免了变量传输过程中的损耗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "broadcast_var= sc.broadcast(array([1,2,3,4]))\n",
    "print(broadcast_var.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.parallelize()\n",
    "从list或者turple生成新的rdd，两个参数为(变量,分区数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sc.parallelize([['a',1],['b',2],['b',3],['c',4],['a',5],['b',6]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a', 1], ['b', 2], ['b', 3]], [['c', 4], ['a', 5], ['b', 6]]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.map()\n",
    "对RDD中每个元素执行依次转换操作，传入一个函数类型的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_key_value = input_data.map(lambda x:(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('b', 3), ('c', 4), ('a', 5), ('b', 6)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_key_value.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.filter(f)\n",
    "过滤函数，选出符合f函数判断的元素，组成新的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zhangxin', 'yanglele', 'zhangyuanyang', 'zhangxiaofeng']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sc.parallelize(['zhangxin','yanglele','zhangyuanyang','zhangxiaofeng'])\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=words.count()\n",
    "print('the RDD counts is->%i' %count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_new=words.filter(lambda x:'zhang' in x)\n",
    "words_new.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_map=words.map(lambda x:x+'good')\n",
    "words_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zhangxinyanglelezhangyuanyangzhangxiaofeng'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adding=words.reduce(add)\n",
    "adding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.reduceByKey(f)\n",
    "针对不同的key，对其值进行f函数的操作，f函数传入两个参数，分别为前一个值或者计算后的结果和后一个值，直到同一个key对应的值全部被计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_value = input_key_value.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('b', 11), ('c', 4)], [('a', 6)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_value.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapPartitions(f, preservesPartitioning=False)\n",
    "与map不同，map是对每一个元素用函数作用；而mapPartitions是\n",
    "对每一个分区用一个函数去作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(iterator):yield sum(iterator)\n",
    "xMP = x.mapPartitions(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [5]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xMP.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapPartitionsWithIndex\n",
    "与mapPartition相比，mapPartitionWithIndex能够保留分区索引，\n",
    "函数的传入参数也是分区索引和iterator构成的键值对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[[(0, 1)], [(1, 5)]]\n",
      "[[1], [5]]\n"
     ]
    }
   ],
   "source": [
    "x=sc.parallelize([1,2,3],2)\n",
    "def f1(partitionIndex,iterator):yield (partitionIndex,sum(iterator))\n",
    "def f2(partitionIndex,iterator):yield sum(iterator)\n",
    "xMP1=x.mapPartitionsWithIndex(f1)\n",
    "xMP2=x.mapPartitionsWithIndex(f2)\n",
    "print(x.glom().collect())\n",
    "print(xMP1.glom().collect())\n",
    "print(xMP2.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.zip(other)\n",
    "将第一个rdd的元素作用键，第二个rdd的元素作为值，组成新rdd的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'A', 'A']\n",
      "[66, 65, 65]\n",
      "[('B', 66), ('A', 65), ('A', 65)]\n"
     ]
    }
   ],
   "source": [
    "x=sc.parallelize(['B','A','A'])\n",
    "y=x.map(lambda x:ord(x))\n",
    "z=x.zip(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.keyBy(f)\n",
    "为rdd中的每个元素按照函数f生成一个键，新rdd的元素以元组形式存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[(1, 1), (4, 2), (9, 3)]\n"
     ]
    }
   ],
   "source": [
    "x=sc.parallelize([1,2,3])\n",
    "y=x.keyBy(lambda x:x**2)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.foreach(f)\n",
    "对RDD中的每个元素使用函数来作用,由于是直接对每个元素操作并产生结果，\n",
    "所以得到的结果不是rdd,而是普通python对象。这与foreachPartition不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "rdd_data=sc.parallelize([1,2,3,4,5],2)\n",
    "print(rdd_data.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    print(x)\n",
    "list_new=rdd_data.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "f(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "3\n",
      "2\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputData=sc.parallelize([1,2,3])\n",
    "def f(x):#定义一个将内容追加于文件末尾的函数\n",
    "    with open('./example.txt','a+') as fl:\n",
    "        print(x,file=fl)\n",
    "\n",
    "open('./example.txt','w').close()#操作之前先关闭之前可能存在的对该文件的写操作\n",
    "y=inputData.foreach(f)\n",
    "print(y)\n",
    "\n",
    "with open('./example.txt') as fl:\n",
    "    print(fl.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.groupByKey(numPartitions=None, partitionFunc=)\n",
    "原rdd为键值对，groupByKey()则将原rdd的元素相同键的值编进一个sequence\n",
    "（不知道与list和iterator的不同有多大，可以暂时当成iterator看）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 1), ('a', 1)]\n",
      "[('b', <pyspark.resultiterable.ResultIterable object at 0x00000287F6768470>), ('a', <pyspark.resultiterable.ResultIterable object at 0x00000287F67684A8>)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rddGp=rdd.groupByKey()\n",
    "print(rdd.collect())\n",
    "print(rddGp.collect())\n",
    "#从结果看，确实是将键相同的值编到一个序列里了，但类型很奇怪。这样看没有什么用处。\n",
    "#但是后面可以接其他函数，一般都接mapValues(f),这样就可以完成按对值的一些操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 1), ('a', 2)]\n",
      "[('b', [1]), ('a', [1, 1])]\n",
      "[('b', [2]), ('a', [2, 2])]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    a=list(x)#直接使用x会报错，说明sequence并不能用for\n",
    "    for i in range(len(a)):\n",
    "        a[i]=a[i]*2\n",
    "    return a\n",
    "\n",
    "gpMp1=rddGp.mapValues(len)\n",
    "gpMp2=rddGp.mapValues(list)\n",
    "gpMp3=rddGp.mapValues(f)\n",
    "print(gpMp1.collect())\n",
    "print(gpMp2.collect())\n",
    "print(gpMp3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.groupBy(f, numPartitions=None, partitionFunc=)\n",
    "groupBy()的用法与groupByKey相似，但传入参数多了f,传入的函数f可以把它当成用来生成新的key的。\n",
    "它也围绕这个潜在的key将值编进一个序列。可以看得出来，它比groupByKey更灵活。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, <pyspark.resultiterable.ResultIterable object at 0x00000287F675BEB8>), (1, <pyspark.resultiterable.ResultIterable object at 0x00000287F675BEF0>)]\n",
      "[(0, [2, 8]), (1, [1, 1, 3, 5])]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result=rdd.groupBy(lambda x:x%2)#按余数来分组\n",
    "#后面紧接着一般是mapValues函数做进一步处理，这里我们直接获取该数据而不做进一步处理。\n",
    "print(result.collect())\n",
    "#显示保存在<pyspark.resultiterable.ResultIterable object at ...>中元素\n",
    "resultGp=[(x,sorted(y)) for (x,y) in result.collect()]\n",
    "print(resultGp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.reduce(f)\n",
    "reduce函数是将rdd中的每个元素两两之间按函数f进行操作，然后再结果再两两之间按f进行操作，一直进行下去，即所谓的shuffle过程。\n",
    "reduce得到的结果是普通的python对象，而不是rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_re = sc.parallelize([i for i in range(1,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_re.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.keys()\n",
    "原rdd的元素为键值对，返回原rdd元素的键为元素的rdd\n",
    "\n",
    "#### rdd.values()\n",
    "原rdd的元素为键值对，返回原rdd元素的值为元素的rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_k_v = sc.parallelize([(i,i**3) for i in range(1,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_k = ex_k_v.keys()\n",
    "ex_v = ex_k_v.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[1, 8, 27, 64]\n"
     ]
    }
   ],
   "source": [
    "print(ex_k.collect())\n",
    "print(ex_v.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fold(zeroValue, op)[source]\n",
    "与partitionBy很像，只不过有一个起始值。fold函数是按分区对每个元素进行操作，即先每个元素与起始值按op进行操作，得到的结果再两两之间按op操作，\n",
    "一直进行下去得到分区结果，然后再将分区结果按op操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "x=sc.parallelize([1,2,3],2)\n",
    "y=x.fold(1,lambda x,y:x+y)\n",
    "print(x.collect())\n",
    "#结果为：[1, 2, 3]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD.aggregate(zeroValue, seqOp, combOp)\n",
    "aggregate与fold相似又很不同。seqOp操作会聚合各分区中的元素，然后combOp操作把所有分区的聚合结果再次聚合，两个操作的初始值都是zeroValue. \n",
    "seqOp的操作是遍历分区中的所有元素(T)，zeroValue跟第一个T做操作，结果再作为与第二个T做操作的zeroValue，直到遍历完整个分区。combOp操作是把各分区聚合的结果，\n",
    "再聚合,zeroValue与第一个分区结果聚合，聚合结果相当于新的zeroValue，再与第二个分区结果聚合，一直进行下去。\n",
    "aggregate函数返回一个跟RDD不同类型的值。因此，需要一个操作seqOp来把分区中的元素T合并成一个U，另外一个操作combOp把所有U聚合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n",
      "(24, 12)\n"
     ]
    }
   ],
   "source": [
    "seqOp=(lambda x,y:(x[0]+y,x[1]+1))\n",
    "combOp=(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "x=sc.parallelize([1,2,3,4,5,6],2)\n",
    "print(x.glom().collect())\n",
    "#结果为：[[1, 2, 3], [4, 5, 6]]\n",
    "y=x.aggregate((1,2),seqOp,combOp)\n",
    "print(y)\n",
    "#结果为：(24, 12)\n",
    "\n",
    "#计算过程如下：\n",
    "#（1，2）--》（1+1，2+1）-->(2+2,3+1)-->(4+3，4+1)-->(7,5)；\n",
    "#（1，2）--》（1+4，2+1）-->(5+5,3+1)-->(10+6，4+1)-->(16,5)；\n",
    "#（1，2）--》（1+7，2+5）-->(8+16,7+5)-->(24，12)；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD.aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=)\n",
    "aggregate是按分区进行，而aggregateByKey是按键来进行，但是zeroValue与aggregate中的用法很不一样，这里的zeroValue是一个值，它即可以跟这样键聚合，\n",
    "也可以跟那个键聚合，而且zeroValue必须与键内聚合时定义的形式一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('B', 3), ('A', 4), ('A', 5), ('A', 6)]\n",
      "[('B', [7, (1, 1), 7, (2, 4), (3, 9)]), ('A', [7, (4, 16), 7, (5, 25), (6, 36)])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize([('B', 1), ('B', 2), ('B', 3), ('A', 4), ('A', 5),('A', 6)])\n",
    "zeroValue=[7]\n",
    "mergeVal=(lambda aggregated,el:aggregated+[(el,el**2)])\n",
    "mergeComb=(lambda agg1,agg2:agg1+agg2)\n",
    "y=x.aggregateByKey(zeroValue,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "#结果为：[('B', 1), ('B', 2), ('B', 3), ('A', 4), ('A', 5), ('A', 6)]\n",
    "print(y.collect())\n",
    "#结果为：[('A', [7, (4, 16), (5, 25), (6, 36)]), ('B', [7, (1, 1), (2, 4), (3, 9)])]\n",
    "#计算过程如下：\n",
    "#('B', [7]);('B', (1,1**2))-->('B', [7,(1,1)])-->('B', [7,(1,1)])；('B', (2,2**2))-->('B', [7,(1,1),(2,4)])...-->\n",
    "#[('B', [7, (1, 1), (2, 4), (3, 9)])]\n",
    "#同时'A'也进行这样的过程\n",
    "#[('B', [7, (1, 1), (2, 4), (3, 9)])];[('A', [7, (4, 16), (5, 25), (6, 36)])]-->\n",
    "#[('A', [7, (4, 16), (5, 25), (6, 36)]), ('B', [7, (1, 1), (2, 4), (3, 9)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "zx=sc.parallelize([(x,x*2) for x in range(1,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "zxx = sc.parallelize([x for x in range(1,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 4, mean: 2.5, stdev: 1.118033988749895, max: 4.0, min: 1.0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zxx.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zxx.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zxx.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2, 4, 3, 6, 4, 8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zx_x = zx.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd.countByValue()\n",
    "统计rdd所有元素中各元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 2, 3: 2, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "inputData=sc.parallelize([1,3,1,2,3])\n",
    "inputCountBV=inputData.countByValue()\n",
    "print(inputCountBV)\n",
    "#结果为：defaultdict(<class 'int'>, {1: 2, 2: 1, 3: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipe(command, env=None, checkCode=False)\n",
    "通过管道向后面环节输出command处理过的结果，具体功能就体现在command，command为linux命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sda=sc.parallelize(['1', '2', '2', '3']).pipe('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Ba', 'C', 'DA']\n"
     ]
    }
   ],
   "source": [
    "#再比如正则匹配来筛选\n",
    "x=sc.parallelize(['A','Ba','C','DA'])\n",
    "y=x.pipe('grep -i \"A\"')\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "#结果为：['A', 'Ba', 'C', 'DA']\n",
    "#['A', 'Ba', 'DA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
